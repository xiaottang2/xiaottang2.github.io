<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Xiaoting Tang&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="My blog. Technology about deep learning and data visualization">
<meta property="og:type" content="website">
<meta property="og:title" content="Xiaoting Tang's Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Xiaoting Tang's Blog">
<meta property="og:description" content="My blog. Technology about deep learning and data visualization">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Xiaoting Tang's Blog">
<meta name="twitter:description" content="My blog. Technology about deep learning and data visualization">
  
    <link rel="alternate" href="/atom.xml" title="Xiaoting Tang&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Xiaoting Tang&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-question-diff-privacy" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/31/question-diff-privacy/" class="article-date">
  <time datetime="2016-10-31T09:38:35.000Z" itemprop="datePublished">2016-10-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/31/question-diff-privacy/">question_diff_privacy</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Recently I am working on a research project about bringing privacy into deep learning. The basic idea is when we are training our model, usually in Stochastic Gradient Descent, instead of changing the weights in the opposite direction of the <strong>gradients</strong>(which is what we usually do), we use a noised version of gradients.</p>
<p>Here I cite the differential privacy algorithm from <a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45428.pdf" target="_blank" rel="external">Martin Abadi et al.</a>.</p>
<p><img src="https://www.dropbox.com/s/93cp4y7w0946db4/Screenshot%202016-10-31%2019.45.42.png?raw=1" alt=""></p>
<p>Pay attention to <strong>Add noise</strong>, where we sample a noise from a gaussian distribution to every gradient in the network of the current example $i$.</p>
<p>According to the this paper and this <a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf" target="_blank" rel="external">book</a>, the consequence of this algorithm is that it makes the gradient update a $(\epsilon,\delta)$ differential private mechanism. But what does that mean?</p>
<p>It means for a mechanism $\mathcal{M}$ and two two different datasets $d$ and $d’$, which differs only in one entity(think <strong>row</strong> in a spreadsheet) – one has an entity and the other does not, the following equation holds.</p>
<p>$$Pr[\mathcal{M}(d)=\xi]\leq e^\epsilon Pr[\mathcal{M}(d’)=\xi]+\delta$$</p>
<p>I felt the same way you felt about this equation – what does it say? You may have a guess that it is about probability, you are right, but to know better than average, let’s give it a concrete example.</p>
<hr>
<p>Suppose a hospital has 1000 records of blood sugar levels of patients who have diabetics, which is supposed to be private. However, in order to facilitate medical research, doctors decide to release the data to the public in a way they think it’s safe – release only the average blood sugar level. But is it really safe? Potentially not.</p>
<p>What the doctors haven’t foreseen is that by comparing the average result in different time, a malicious person can extract personal information from the average. </p>
<p>K is a data digger whose interest is digging other people’s private info and he has a lot of friends in different industry who can offer him such information. K has a friend in the hospital, but he can only know how many records are in the dataset, and who are in the dataset. They way K can extract information is that he query the medical record system every day, and he asked his friend to send him a message whenever the names in the record changes. One day K received a message, telling him that another patient’s data has been uploaded in the record. K queried again, get the new average value and he used the following equation to get the blood sugar level of the new patient</p>
<p>$ x_{new_patient} = {avg}_2\times1001 - {avg}_1\times1000 $</p>
<p>Now that you see how insecure an aggregate function can be, let’s find out what the equation $$Pr[\mathcal{M}(d)=\xi]\leq e^\epsilon Pr[\mathcal{M}(d’)=\xi]+\delta$$ has to say.</p>
<p>According to the theory(see the book mentioned above, the first chapter), a function $f$ can be protected by a mechanism $\mathcal{M}$ like this: $\mathcal{M}(d) = f(d) + \mathcal{N}(0, \sigma S_f)$, where $\mathcal{N}$ is a normal distribution:<br>$$ \mathcal{N}(\delta, \sigma) = \frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(x-\delta)^2}{2\sigma}} $$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/31/question-diff-privacy/" data-id="ciuy2tjke0005ysaygsqw9dxe" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    [object Promise]
  
    <article id="post-data-sceince-portfolio-machine-learning-project" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/09/21/data-sceince-portfolio-machine-learning-project/" class="article-date">
  <time datetime="2016-09-21T13:38:24.000Z" itemprop="datePublished">2016-09-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/21/data-sceince-portfolio-machine-learning-project/">构建数据科学的作品集：从机器学习项目开始</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>##构建数据科学的作品集：从机器学习项目开始</p>
<p>本文是如何建立数据科学作品的第三步曲。如果你喜欢并想要知本系列的下一篇文章什么时候发布，可以<a href="https://www.dataquest.io/blog/data-science-portfolio-machine-learning/#email-signup" target="_blank" rel="external">在文末订阅</a>。</p>
<p>数据科学公司在决定是否雇佣的时候越来越看重应聘者的作品集。其中一个原因是作品集是最好判断一个人的真实技能的方法。好消息是你的作品集其实完全在你掌控之中。如果你花精力在上面，你就可以做出一个让公司刮目相看的作品集。</p>
<p>建立一个高质量的作品的第一步是要知道要展示哪些技能。公司想要数据科学家拥有的最主要的技能，也就是他们想你的作品集展示的最主要的技能，有下面这些：</p>
<ul>
<li>交流能力</li>
<li>与他人协作的能力</li>
<li>技术竞争力</li>
<li>能合理解释数据的能力</li>
<li>积极主动的动力和能力</li>
</ul>
<p>任何好的作品集都是由数个项目构成的，每一个项目可能展示1-2个上面提到的技能。本文是讲述如何建立一个丰满的数据科学作品集的第三步曲。本文将涵盖如何制作你作品集里的第二个项目，和如何从头到尾地建立一个机器学习项目。在最后，你会拥有一个可以展示你合理解释数据能力和技术竞争力的项目。如果你想一窥项目全貌的话，<a href="https://github.com/dataquestio/loan-prediction" target="_blank" rel="external">这里</a>是完整的项目文件。</p>
<p>#一个从头到尾的项目</p>
<p>作为一个数据科学家，有时候你会被叫去拿一个数据集然后想方设法用数据<a href="https://www.dataquest.io/blog/data-science-portfolio-project/" target="_blank" rel="external">说一个故事</a>。这些时候，良好的沟通和理清你的思路是非常重要的。像我们在之前提到的Jupyter notebook这样的工具就能很好地帮助你做这件事。一个预期结果是你最终递交出一个报告，或者一个总结你发现的文档。</p>
<p>然而，也有时候你会被叫去做一个有运营价值的项目。一个有运营价值的项目会直接影响一个公司的日常运营，而且经常会被许多人使用多次。一个像这样的任务可能会是“设计一个可以预测我们用户变动率的算法”， 或者是“创建一个可以自动给我们的文章标签的模型”。在这样的情况下，讲故事的能力就没有技术竞争力重要了。你需要能够拿到一个数据集，理解它，然后写一些可以处理这些数据的脚本。经常很重要的是这些脚本要花最小的系统资源，比如内存，还要运行得很快。很常见的事是这些脚本会被运行多次，所以最终的交付品就变成了这些脚本自身，而不是报告。这些成果经常都会和运营流程综合在一起，甚至可能是直接面对用户的。</p>
<p>建立从头到尾的项目的主要构成有：</p>
<ul>
<li>理解整个项目环境</li>
<li>探索数据并找到其中的细微差别</li>
<li>建立一个结构良好的项目，使得其和运营流程结合很简单</li>
<li>写出既运行快又占用最少系统资源的高性能代码</li>
<li>为你的代码的安装和使用写出良好的文档，让其他人也能使用</li>
</ul>
<p>为了高效地建立这样的项目，我们会需要和许多文件打交道。我们非常推荐使用像<a href="https://atom.io/" target="_blank" rel="external">Atom</a>的文档编辑器，或者像<a href="https://www.jetbrains.com/pycharm/" target="_blank" rel="external">PyCharm</a>一样的IDE。这些工具允许你在不同文件间跳转，并且可以编辑不同类型的文件，比如markdown文件，Python文件和csv文件。给你的代码建立结构，使得版本管理和上传到像<a href="https://github.com/" target="_blank" rel="external">Github</a>这样的代码协作工具变得简单，也是很有帮助的。</p>
<p><img src="https://www.dataquest.io/blog/images/end_to_end/github.png" alt=""><br><em>在Github上的该项目</em></p>
<p>在本文中，我们会使用比如<a href="http://pandas.pydata.org/" target="_blank" rel="external">Pandas</a>和<a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn</a>这样的库。我们会特别多地用到Pandas的<a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html" target="_blank" rel="external">DataFrame</a>，这会使得读取数据和用Python处理扁平数据更简单。</p>
<p>#找到好的数据集</p>
<p>一个对于有头有尾的项目有利的数据集可能会很难找。这个数据集需要足够大，这样才能体现内存和性能的限制。它也需要潜在地对运营方面有用。举个例子，<a href="https://collegescorecard.ed.gov/data/" target="_blank" rel="external">这个数据集</a>，它包含了美国大学的招生条件，毕业率，和毕业生未来收入的数据。这就是一个可以用来讲故事的很好的数据集。然而，如果你深入地去想，你就会发现这里面没有足够的细节来建立一个好项目。举例说，你可以告诉别人如果他们去某些（好）大学，他们未来的潜在收入就会更高，但是这只需要一个很快的查找比较就可以完成，而没有足够的空间去展示你的技术竞争力。你也可以发现如果大学有更高的入学条件，它们的毕业生就更有可能获得高薪，但这些就更偏向于讲故事，而非运营技术了。</p>
<p>当你有GB以上的数据量时，或者当你想要预测一些数据细节，需要对数据集进行一些算法运算时，内存和性能限制就会逐渐凸现出来。</p>
<p>一个好的运营数据集可以让你建立一系列的脚本，这些脚本可以帮助你给数据做变形，从而可以回答一些动态的问题。股票价格就是一个很好的数据集。你可以根据这些数据在第二天预测当天的股价走势，并且在股市关闭的时候把新数据提供给算法。这可以帮助你执行交易，甚至可以帮你获取利润。这就不是在讲故事了 – 这就是直接在产生价值。</p>
<p>一些能够找到好数据集的地方：</p>
<ul>
<li><a href="https://reddit.com/r/datasets" target="_blank" rel="external">/r/datasets</a> – 一个有着上百有趣的数据集的subreddit</li>
<li><a href="https://cloud.google.com/bigquery/public-data/#usa-names" target="_blank" rel="external">Google Public Datasets</a> – 一些在Google BigQuery上的公共数据集</li>
<li><a href="https://github.com/caesar0301/awesome-public-datasets" target="_blank" rel="external">Awesome datasets</a> – 一个托管在Github上的数据集清单</li>
</ul>
<p>当你在看这些数据集的时候，想一想一个人如果有这些数据集，可能会回答些什么问题，然后再想想这些问题是否是一次性的（“S&amp;P 500和房价的相关性是怎样的？”），或是不间断的（“你能预测股票价格吗？”）。这里的关键在于找到那些不间断的问题，这些问题会需要同样的代码在不同的时间用不同的数据去运行。</p>
<p>在这篇文章里面，我们选择<a href="http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html" target="_blank" rel="external">Fannie Mae贷款数据</a>。Fannie Mae是一个由美国政府资助的从贷方手里购买房贷的企业。它购买房贷之后，会把这些房贷打包到一些由房贷支撑的证券里，再卖出去。这样就帮助了贷方可以贷出更多的房贷，并给市场创造了更大的流动性。这，从理论上说，就会产生更多的房屋业主，进而产生更好的房贷政策。然而从借方的角度来看，事情大致并没有什么不同。</p>
<p>Fannie Mae 开放了两种数据 – 它得到的房贷数据，和这些房贷一段时间后的表现情况数据。在最理想的情况下，一个人从贷方贷了款，然后一直还钱，知道贷款还清。然而，当借方有几次没有还款，就可能会导致他失去抵押品赎回权。这时，银行就会获得房屋的所有权，因为房贷不能还清。Fannie Mae记录了哪些房贷没有还，哪些房贷需要取消借方的抵押品赎回权。这个数据一个季度发布一次，最新的数据会是去年的。在撰写本文档 时候，最近的数据集是<code>2015</code>的第一个季度。</p>
<p>当Fannie Mae得到房贷时发布的收购信息，涵盖了许多关于借方的信息，包括信用记录，和关于借方的房贷和房屋的信息。在房贷借出的每个季度发布的房贷表现信息里，涵盖了借方的支付信息，和抵押权的状态。一个房贷的表现信息里可能有很多行。你可以这么想这个事，收购信息告诉你Fannie Mae现在控制了房贷，表现信息则包括了一系列房贷的状况更新。一种状态可能会告诉我们这笔贷款在今年的某个季度借方抵押权被取消了。</p>
<p><img src="https://www.dataquest.io/blog/images/end_to_end/foreclosure.jpg" alt=""><br><em>一个借方失去了抵押品赎回权（止赎）的房子正在被卖</em></p>
<p>#选一个角度<br>有了Fannie Mae数据集之后，我们可以从几个角度出发。我们可以：</p>
<ul>
<li>尝试预测一个止赎了的房屋的售价</li>
<li>预测一个借方的还款历史</li>
<li>收购时，计算出一个房贷评分</li>
</ul>
<p>重要的事是要坚持一个单一的角度。想专注于太多事情会很难做成一个优秀的项目。也很重要的是选一个有足够细节的角度。这里是一些没有多少细节的角度：</p>
<ul>
<li>找到哪间银行卖给Fannie Mae最多止赎的房贷</li>
<li>找到借方信用记录的趋势</li>
<li>探索哪些房屋类型最经常止赎</li>
<li>探索房贷金额和止赎售价的关系</li>
</ul>
<p>上述的这些角度都很有趣，而且如果我们关注讲故事的话也会很棒，但对于一个运营性的项目来说就没那么好了。</p>
<p>有了Fannie Mae数据集，我们会尝试预测一个房贷是否会被止赎 – 仅仅使用收购房贷时的数据。实际来说，我们会为每一份房贷创建一个“分数”，这个分数会告诉Fannie Mae是否应该购买这份房贷。这会给我们一个良好的基础，也是一个很棒的作品。</p>
<p>#理解数据</p>
<p>让我们很快地看一看原始的数据文件。这里是2012年第一季度的收购数据的前几行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">100000853384|R|OTHER|4.625|280000|360|02/2012|04/2012|31|31|1|23|801|N|C|SF|1|I|CA|945||FRM|</div><div class="line">100003735682|R|SUNTRUST MORTGAGE INC.|3.99|466000|360|01/2012|03/2012|80|80|2|30|794|N|P|SF|1|P|MD|208||FRM|788</div><div class="line">100006367485|C|PHH MORTGAGE CORPORATION|4|229000|360|02/2012|04/2012|67|67|2|36|802|N|R|SF|1|P|CA|959||FRM|794</div></pre></td></tr></table></figure></p>
<p>这里是2012年第一季度的表现数据的前几行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">100000853384|03/01/2012|OTHER|4.625||0|360|359|03/2042|41860|0|N||||||||||||||||</div><div class="line">100000853384|04/01/2012||4.625||1|359|358|03/2042|41860|0|N||||||||||||||||</div><div class="line">100000853384|05/01/2012||4.625||2|358|357|03/2042|41860|0|N||||||||||||||||</div></pre></td></tr></table></figure>
<p>在讲代码之前，花点时间去理解数据是很有用的。在运营型项目里这点更重要 – 因为我们不会互动式地去探索数据，找寻某些细节变得更难，除非我们一开始就找到它们。这种情况下，第一步就是去Fannie Mae网站上读一读它的材料：</p>
<ul>
<li><a href="http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html" target="_blank" rel="external">简介</a></li>
<li><a href="https://loanperformancedata.fanniemae.com/lppub-docs/lppub_glossary.pdf" target="_blank" rel="external">词汇表</a></li>
<li><a href="https://loanperformancedata.fanniemae.com/lppub-docs/lppub_faq.pdf" target="_blank" rel="external">常见问题</a></li>
<li><a href="https://loanperformancedata.fanniemae.com/lppub-docs/lppub_file_layout.pdf" target="_blank" rel="external">收购和表现文件里的列</a></li>
<li><a href="https://loanperformancedata.fanniemae.com/lppub-docs/acquisition-sample-file.txt" target="_blank" rel="external">收购数据样本文件</a></li>
<li><a href="https://loanperformancedata.fanniemae.com/lppub-docs/performance-sample-file.txt" target="_blank" rel="external">表现数据样本文件</a></li>
</ul>
<p>在阅读了材料之后，我们知道了一些可以帮助我们的关键信息：</p>
<ul>
<li>每个季度都有一个收购文件和表现文件，从<code>2000</code>年到现在。数据距离现在有1年的间隔，所以最近的数据是2015年</li>
<li>这些文件是文档形式，用<code>|</code>作为分隔符</li>
<li>这些文件没有头文档，但是我们有所有列名称的一个列表</li>
<li>全部加起来，这些文件包括了<code>2.2</code>千万的房贷</li>
<li>因为表现文件涵盖了之前的房贷信息，所以早些时候的房贷会有更多的表现数据（举个例子，<code>2014</code>年收购的房贷不会有太多表现信息）</li>
</ul>
<p>这一点点的信息会帮我们在结构化项目和处理数据时节省一大笔时间。</p>
<p>#结构化项目</p>
<p>在我们开始下载和探索数据之前，我们如何把数据结构化是非常重要的。在创建项目的时候，我们主要的目标是：</p>
<ul>
<li>创造一个可行的解决方法</li>
<li>拥有一个运行快且消耗最少资源的解决方案</li>
<li>让他人可以很容易地扩展我们的工作</li>
<li>让他人可以容易地理解我们的代码</li>
<li>写的代码越少越好</li>
</ul>
<p>为了达到这些目标，我们也需要结构化我们的数据。一个有良好结构的项目跟从以下的规范：</p>
<ul>
<li>数据文件和源代码分开放置</li>
<li>原始数据和生成的数据分开放置</li>
<li>有一个<code>README.md</code>文件，可以帮助人们安装并使用这个项目</li>
<li>有一个<code>requirements.txt</code>文件，该文件包括这个项目所需的所有模块</li>
<li>有一个<code>settings.py</code>文件，包括所有其他文件里面的设置<ul>
<li>举例来说，如果你有很多Python脚本都读取同一个文件，就不如让它们都导入<code>settings</code>并从这一个地方来得到文件</li>
</ul>
</li>
<li>有一个<code>.gitignore</code>文件，来防止一些特别大的或者私密的文件被commit</li>
<li>把我们的任务分成几步，并分别放在几个可以单独执行的文件里<ul>
<li>举例来说， 我们可能用一个文件读取数据，一个文件建立特征，一个文件执行预测</li>
</ul>
</li>
<li>储存中间值。举例来说，一个脚本可能会输出一个文件，这个文件又会被另外一个脚本读取<ul>
<li>这就使得我们可以在数据处理的流程中做一些改动，而又不需要重新计算所有的东西</li>
</ul>
</li>
</ul>
<p>一会，我们的文件就会呈现以下结构：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">loan-prediction</div><div class="line">├── data</div><div class="line">├── processed</div><div class="line">├── .gitignore</div><div class="line">├── README.md</div><div class="line">├── requirements.txt</div><div class="line">├── settings.py</div></pre></td></tr></table></figure></p>
<p>#创建初始文件</p>
<p>在开始时，我们要创建一个<code>loan-prediction</code>文件夹。在这个文件夹里，我们需要创建一个<code>data</code>文件夹和一个<code>processed</code>文件夹。第一个用来储存我们的原始数据，第二个用来储存所有中间值。</p>
<p>接着，我们创建一个<code>.gitignore</code>文件。一个<code>.gitignore</code>文件会确保一些文件会被git忽略，并不会被push到Github上。一个这类文件很好的例子就是OSX在每个文件夹里创建的<code>.DS_Store</code>文件。要入门<code>.gitignore</code>文件，可以参考<a href="https://github.com/github/gitignore/blob/master/Python.gitignore" target="_blank" rel="external">这里</a>。我们也想忽略一些体积太大的文件，而且Fannie Mae的条款并不允许我们二次发布这些文件，所以我们应该在<code>.gitignore</code>文件最后加上这两行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">data</div><div class="line">processed</div></pre></td></tr></table></figure>
<p><a href="https://github.com/dataquestio/loan-prediction/blob/master/.gitignore" target="_blank" rel="external">这里</a>是本项目的<code>.gitignore</code>文件。</p>
<p>接着，我们需要创建<code>README.md</code>，这会帮助人们理解这个项目。<code>.md</code>代表这个文件是markdown格式。Markdown能让你直接用文字书写，但是如果你想要的话，也可以调用一些好看的排版格式。<a href="https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet" target="_blank" rel="external">这里</a>是一个markdown的指南。如果你在Github上上传了一个叫<code>README.md</code>的文件，Github就会自动处理这个markdown,然后把它作为主页展示给浏览者。<a href="https://github.com/dataquestio/loan-prediction" target="_blank" rel="external">这</a>是一个例子。</p>
<p>暂时来说，我们只需要把一段简短的描述放在<code>README.md</code>里面：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Loan Prediction</div><div class="line">-----------------------</div><div class="line"></div><div class="line">Predict whether or not loans acquired by Fannie Mae will go into foreclosure.  Fannie Mae acquires loans from other lenders as a way of inducing them to lend more.  Fannie Mae releases data on the loans it has acquired and their performance afterwards [here](http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html).</div></pre></td></tr></table></figure>
<p>现在我们创建<code>requirements.txt</code>文件。这会帮助其他人安装我们的项目。我们还不知道我们具体需要哪些库，但这是一个好的起点：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">pandas</div><div class="line">matplotlib</div><div class="line">scikit-learn</div><div class="line">numpy</div><div class="line">ipython</div><div class="line">scipy</div></pre></td></tr></table></figure>
<p>以上是用Python作数据分析的最常用的几个库，我们也可以假设我们在之后会用到它们。<a href="https://github.com/dataquestio/loan-prediction/blob/master/requirements.txt" target="_blank" rel="external">这是</a>本次项目的requirements文件。</p>
<p>在创建<code>requirements.txt</code>之后，你应该安装这些模块。在本文中我们会使用<code>Python 3</code>。如果你还没有安装Python，你应该尝试使用<a href="https://www.continuum.io/downloads" target="_blank" rel="external">Anaconda</a>，这是一个可以安装上述所有模块的Python安装器。</p>
<p>最后，我们可以暂时创建一个空白的<code>settings.py</code>文件，因为我们还没有为项目设置任何东西。</p>
<p>#获得数据</p>
<p>一旦我们有了整个项目的框架，我们就可以开始获取原始数据了。</p>
<p>Fannie Mae 对数据下载有一些限制，所以你得先注册一个账号。下载页面在<a href="https://loanperformancedata.fanniemae.com/lppub/index.html" target="_blank" rel="external">这里</a>。注册完账户后，你就可以想下载多少贷款数据就下载多少了。文件是zip格式，解压之后也挺大的。</p>
<p>为了这篇文章，我们会把<code>Q1 2012</code>至<code>Q1 2015</code>之间的所有数据都下载下来。之后我们需要解压文件。解压之后，删除原始的<code>.zip</code>文件。最后，<code>loan-prediction</code>文件夹的会呈现以下结构：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">loan-prediction</div><div class="line">├── data</div><div class="line">│   ├── Acquisition_2012Q1.txt</div><div class="line">│   ├── Acquisition_2012Q2.txt</div><div class="line">│   ├── Performance_2012Q1.txt</div><div class="line">│   ├── Performance_2012Q2.txt</div><div class="line">│   └── ...</div><div class="line">├── processed</div><div class="line">├── .gitignore</div><div class="line">├── README.md</div><div class="line">├── requirements.txt</div><div class="line">├── settings.py</div></pre></td></tr></table></figure>
<p>下载完数据之后，你可以用<code>head</code>和<code>tail</code>的shell命令去观察文件里面的前几行和后几行。你有看到一些我们不需要的列吗？在做这件事情的时候可以参考一下<a href="https://loanperformancedata.fanniemae.com/lppub-docs/lppub_file_layout.pdf" target="_blank" rel="external">pdf列名称</a></p>
<p>#读取数据</p>
<p>现在有两个问题，使得我们直接利用现在的数据：</p>
<ul>
<li>收购和表现数据集被分散在了许多文件里</li>
<li>所有文件都缺少头文档</li>
</ul>
<p>在我们处理这些数据之前，我们需要把集中所有的收购数据到一个文件，和集中所有的表现数据到一个文件。每个文件只需要包含我们关心的列的数据，和正常的头文档。这里的一个问题是表现数据特别大，所以可能的话我们得删减一些列。</p>
<p>第一步是在<code>settings.py</code>里面增添一些变量，这些变量会包含到原始数据和中间数据的路径。我们也会加上一点在之后会有用的数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">DATA_DIR = &quot;data&quot;</div><div class="line">PROCESSED_DIR = &quot;processed&quot;</div><div class="line">MINIMUM_TRACKING_QUARTERS = 4</div><div class="line">TARGET = &quot;foreclosure_status&quot;</div><div class="line">NON_PREDICTORS = [TARGET, &quot;id&quot;]</div><div class="line">CV_FOLDS = 3</div></pre></td></tr></table></figure>
<p>把路径放在<code>settings.py</code>里面会使得它们统一在一个地方，使得今后改动它们变得简单。当许多文件都用了同一些变量的时候，把它们放在一起会比分别在每个文件里做改动要简单得多。<a href="https://github.com/dataquestio/loan-prediction/blob/master/settings.py" target="_blank" rel="external">这里</a>是该项目的<code>settings.py</code>文件。</p>
<p>第二步是创建一个叫做<code>assemble.py</code>的文件，这个文件会把所有东西组合成<code>2</code>个文件。当我们运行<code>python assemble.py</code>的时候，我们会在<code>processed</code>文件夹里面得到<code>2</code>个数据文件。</p>
<p>我们一开始先给<code>assemble.py</code>写代码。要开始，我们得给每个文件定义头文档，所以我们需要看一看<a href="https://loanperformancedata.fanniemae.com/lppub-docs/lppub_file_layout.pdf" target="_blank" rel="external">列名称的pdf文档</a>然后给每个收购和表现文件建立一系列列表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line">HEADERS = &#123;</div><div class="line">    &quot;Acquisition&quot;: [</div><div class="line">        &quot;id&quot;,</div><div class="line">        &quot;channel&quot;,</div><div class="line">        &quot;seller&quot;,</div><div class="line">        &quot;interest_rate&quot;,</div><div class="line">        &quot;balance&quot;,</div><div class="line">        &quot;loan_term&quot;,</div><div class="line">        &quot;origination_date&quot;,</div><div class="line">        &quot;first_payment_date&quot;,</div><div class="line">        &quot;ltv&quot;,</div><div class="line">        &quot;cltv&quot;,</div><div class="line">        &quot;borrower_count&quot;,</div><div class="line">        &quot;dti&quot;,</div><div class="line">        &quot;borrower_credit_score&quot;,</div><div class="line">        &quot;first_time_homebuyer&quot;,</div><div class="line">        &quot;loan_purpose&quot;,</div><div class="line">        &quot;property_type&quot;,</div><div class="line">        &quot;unit_count&quot;,</div><div class="line">        &quot;occupancy_status&quot;,</div><div class="line">        &quot;property_state&quot;,</div><div class="line">        &quot;zip&quot;,</div><div class="line">        &quot;insurance_percentage&quot;,</div><div class="line">        &quot;product_type&quot;,</div><div class="line">        &quot;co_borrower_credit_score&quot;</div><div class="line">    ],</div><div class="line">    &quot;Performance&quot;: [</div><div class="line">        &quot;id&quot;,</div><div class="line">        &quot;reporting_period&quot;,</div><div class="line">        &quot;servicer_name&quot;,</div><div class="line">        &quot;interest_rate&quot;,</div><div class="line">        &quot;balance&quot;,</div><div class="line">        &quot;loan_age&quot;,</div><div class="line">        &quot;months_to_maturity&quot;,</div><div class="line">        &quot;maturity_date&quot;,</div><div class="line">        &quot;msa&quot;,</div><div class="line">        &quot;delinquency_status&quot;,</div><div class="line">        &quot;modification_flag&quot;,</div><div class="line">        &quot;zero_balance_code&quot;,</div><div class="line">        &quot;zero_balance_date&quot;,</div><div class="line">        &quot;last_paid_installment_date&quot;,</div><div class="line">        &quot;foreclosure_date&quot;,</div><div class="line">        &quot;disposition_date&quot;,</div><div class="line">        &quot;foreclosure_costs&quot;,</div><div class="line">        &quot;property_repair_costs&quot;,</div><div class="line">        &quot;recovery_costs&quot;,</div><div class="line">        &quot;misc_costs&quot;,</div><div class="line">        &quot;tax_costs&quot;,</div><div class="line">        &quot;sale_proceeds&quot;,</div><div class="line">        &quot;credit_enhancement_proceeds&quot;,</div><div class="line">        &quot;repurchase_proceeds&quot;,</div><div class="line">        &quot;other_foreclosure_proceeds&quot;,</div><div class="line">        &quot;non_interest_bearing_balance&quot;,</div><div class="line">        &quot;principal_forgiveness_balance&quot;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>下一步是定义我们需要保留哪些列。因为我们最终持续关心的房贷只是关于它有没有被止赎，所以我们可以从表现数据里面丢弃很多列（不影响是否止赎的数据）。但是我们需要保留所有收购数据，因为我们想要最大化关于被收购的房贷的信息（因为最终我们是在房贷被收购的时候预测它是否会被止赎）。丢弃一些列可以让我们省下一些磁盘空间和内存，同时也会加速代码的运行速度。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">SELECT = &#123;</div><div class="line">    &quot;Acquisition&quot;: HEADERS[&quot;Acquisition&quot;],</div><div class="line">    &quot;Performance&quot;: [</div><div class="line">        &quot;id&quot;,</div><div class="line">        &quot;foreclosure_date&quot;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>接下来，我们要写一个函数来连接所有的数据集。下面的代码会：</p>
<ul>
<li>导入一些需要的库，包括<code>settings</code></li>
<li>定义一个函数<code>concatenate</code>，它可以：<ul>
<li>拿到<code>data</code>目录里面所有文件的名字</li>
<li>遍历每个文件<ul>
<li>如果文件的格式不对（并不是以我们想要的前缀开始），我们就忽略它</li>
<li>用Pandas的<a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html" target="_blank" rel="external">read_csv</a>函数，用合适的设置，把文件读取到一个<a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html" target="_blank" rel="external">DataFrame</a>里<ul>
<li>把分隔符设置为<code>|</code>使得数据可以正确地读取</li>
<li>数据现在没有头文档，所以把<code>header</code>设置成<code>None</code></li>
<li>把<code>HEADERS</code>字典里的值设置给列的名称，这些会成为我们DataFrame里面的列名称</li>
<li>只把我们加在<code>SELECT</code>里面的列从DataFrame里面选出来</li>
</ul>
</li>
<li>把所有的DataFrame连接在一起</li>
<li>把连接好的DataFrame输出成一个文件</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">import settings</div><div class="line">import pandas as pd</div><div class="line"></div><div class="line">def concatenate(prefix=&quot;Acquisition&quot;):</div><div class="line">    files = os.listdir(settings.DATA_DIR)</div><div class="line">    full = []</div><div class="line">    for f in files:</div><div class="line">        if not f.startswith(prefix):</div><div class="line">            continue</div><div class="line"></div><div class="line">        data = pd.read_csv(os.path.join(settings.DATA_DIR, f), sep=&quot;|&quot;, header=None, names=HEADERS[prefix], index_col=False)</div><div class="line">        data = data[SELECT[prefix]]</div><div class="line">        full.append(data)</div><div class="line"></div><div class="line">    full = pd.concat(full, axis=0)</div><div class="line"></div><div class="line">    full.to_csv(os.path.join(settings.PROCESSED_DIR, &quot;&#123;&#125;.txt&quot;.format(prefix)), sep=&quot;|&quot;, header=SELECT[prefix], index=False)</div></pre></td></tr></table></figure>
<p>我们可以用参数<code>Acquisition</code>和<code>Performance</code>调用两次上面的函数，来把所有的收购和表现文件连接在一起。下面的代码会：</p>
<ul>
<li>只当脚本是在命令性用<code>python assemble.py</code>执行时运行</li>
<li>连接所有文件，并输出成两个文件：<ul>
<li><code>processed/Acquisition.txt</code></li>
<li><code>processed/Performance.txt</code></li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">    concatenate(&quot;Acquisition&quot;)</div><div class="line">    concatenate(&quot;Performance&quot;)</div></pre></td></tr></table></figure>
<p>我们现在有了一个良好划分的<code>assemble.py</code>文件，它既容易运行，又易扩展。像这样把大问题划分成小问题，我们将项目变得更简单。我们把不同文件分离开，定义它们之间的数据结构，而不是用一个脚本做所有的事情。当你在做一个大项目的时候，这样做通常很好，因为这样你就可以更改一些文件而不会产生不可预期的结果。</p>
<p>一旦我们完成了<code>assemble.py</code>的脚本，我们就可以来运行它。你可以在<a href="https://github.com/dataquestio/loan-prediction/blob/master/assemble.py" target="_blank" rel="external">这里</a>找到它。</p>
<p>这就会在<code>processed</code>目录里面输出两个文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">loan-prediction</div><div class="line">├── data</div><div class="line">│   ├── Acquisition_2012Q1.txt</div><div class="line">│   ├── Acquisition_2012Q2.txt</div><div class="line">│   ├── Performance_2012Q1.txt</div><div class="line">│   ├── Performance_2012Q2.txt</div><div class="line">│   └── ...</div><div class="line">├── processed</div><div class="line">│   ├── Acquisition.txt</div><div class="line">│   ├── Performance.txt</div><div class="line">├── .gitignore</div><div class="line">├── assemble.py</div><div class="line">├── README.md</div><div class="line">├── requirements.txt</div><div class="line">├── settings.py</div></pre></td></tr></table></figure>
<p>#在表现数据里进行计算</p>
<p>我们的下一步就是从<code>processed/Performance.txt</code>里面计算一些值。我们想做的就是预测一间房产以后会不会被止赎。为了弄明白这一点，我们只需要看看表现数据里面的房贷是否有一个<code>foreclosure_date</code>。如果<code>foreclosure_date</code>是<code>None</code>，那么这间房产就没有被止赎。我们也需要规避那些在表现数据里没有多少历史数据的房贷，要做到这一点，我们通过计算它们在表现数据里面累计有多少行。这就可以帮我们过滤掉那些没多少历史数据的房贷。</p>
<p>我们可以用下面的方法来思考收购数据和表现数据的关系：</p>
<p><img src="https://www.dropbox.com/s/f0ld1ztm7w5lim6/Screenshot%202016-09-29%2021.56.18.png?raw=1" alt=""></p>
<p>可以看到的是，在收购数据里每一行都在表现数据里对应了多行。在表现数据里面，当止赎发生的时候，当季度的<code>foreclosure_date</code>就会出现，在这之前都应该是空白的。一些贷款从未被止赎，所以与之相关的表现数据里的<code>foreclosure_date</code>都是空白的。</p>
<p>我们需要计算<code>foreclorsure_status</code>，这是一个布尔值，代表一个贷款<code>id</code>是否有被止赎过。我们也要计算<code>performance_count</code>，也就是每个<code>id</code>在表现数据里有多少行。</p>
<p>有几种方法可以计算我们想要的<code>performance_count</code>：</p>
<pre><code>- 我们可以读取所有的表现数据，然后用Pandas的[groupby](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)方法来搞清楚和每个贷款`id`相关联的行数，同时也可以搞清楚这个`id`的`foreclosure_date`有没有不是`None`过。
    - 这样做的好处是实现的语法很简单
    - 这样做的坏处是读取`129236094`行数据会花很多内存，也会极其得慢
- 我们可以读取所有的表现数据，然后用[apply](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html)在收购数据DataFrame身上，从而找到每个`id`的计数
    - 好处是概念上很简单
    - 坏处仍然是读取`129236094`行数据会花很多内存，也会极其得慢
- 我们可以表现数据里的每一行，然后保存一个单独的包含计数的字典
    - 好处是我们不需要把所有数据一起读取进内存，所以这样做会很快，也会优化内存
    - 坏处是我们得花长一点时间来理清概念和实现，而且我们需要手工地解析每一行
</code></pre><p>把所有数据一并加载会花很多内存，所以我们采用第三种方法。我们所要的就是遍历表现数据里面的每一行，并且保存一个包含每个<code>id</code>的计数字典。在字典里面，我们记录下表现数据里面每个<code>id</code>出现了多少次，并且是否<code>foreclosure_date</code>是否为非<code>None</code>过。这就给了我们<code>foreclosure_date</code>和<code>performance_count</code>。</p>
<p>我们要新建一个文件<code>annotate.py</code>，并加进我们用来计算的代码。在下面的代码里面，我们会：</p>
<pre><code>- 导入需要的库
- 定义一个叫做`count_performance_rows`的函数
    - 打开`precessed/Performance.txt`。这不会把文件读取进内存，而仅仅是打开一个文件管理者，它可以帮我们一行一行地读取文件内容
    - 遍历文件里的每一行
        - 遇见分隔符`|`就分割字符串
        - 检查`loan_id`是否在`counts`字典里
            - 如果不在，把它加入`counts`
        - 给`load_id`对应的`performance_count`加1
        - 如果`date`不是`None`，那么我们就知道这笔贷款止赎了，所以设置`foreclosure_status`
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">import settings</div><div class="line">import pandas as pd</div><div class="line"></div><div class="line">def count_performance_rows():</div><div class="line">    counts = &#123;&#125;</div><div class="line">    with open(os.path.join(settings.PROCESSED_DIR, &quot;Performance.txt&quot;), &apos;r&apos;) as f:</div><div class="line">        for i, line in enumerate(f):</div><div class="line">            if i == 0:</div><div class="line">                # Skip header row</div><div class="line">                continue</div><div class="line">            loan_id, date = line.split(&quot;|&quot;)</div><div class="line">            loan_id = int(loan_id)</div><div class="line">            if loan_id not in counts:</div><div class="line">                counts[loan_id] = &#123;</div><div class="line">                    &quot;foreclosure_status&quot;: False,</div><div class="line">                    &quot;performance_count&quot;: 0</div><div class="line">                &#125;</div><div class="line">            counts[loan_id][&quot;performance_count&quot;] += 1</div><div class="line">            if len(date.strip()) &gt; 0:</div><div class="line">                counts[loan_id][&quot;foreclosure_status&quot;] = True</div><div class="line">    return counts</div></pre></td></tr></table></figure>
<p>#得到计算结果</p>
<p>一旦我们创建了我们的counts字典，我们就可以用一个函数抽取出和传入的<code>load_id</code>和<code>key</code>相应的值了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def get_performance_summary_value(loan_id, key, counts):</div><div class="line">    value = counts.get(loan_id, &#123;</div><div class="line">        &quot;foreclosure_status&quot;: False,</div><div class="line">        &quot;performance_count&quot;: 0</div><div class="line">    &#125;)</div><div class="line">    return value[key]</div></pre></td></tr></table></figure>
<p>上面的这个函数会从<code>counts</code>字典里返回适当的值，并且会让我们可以赋予收购数据里每一行一个<code>foreclosure_status</code>和<code>performance_count</code>值。字典里的<a href="https://docs.python.org/3/library/stdtypes.html#dict.get" target="_blank" rel="external">get</a>方法在没有找到key的情况下就会返回一个默认值，所以这就让我们可以区分是否找到了某个key。</p>
<p>#给数据做标记</p>
<p>我们已经给<code>annotate.py</code>加上一些函数了，现在我们就可以开始处理最有价值的部分了。我们需要把收购数据转换成一个机器学习算法可以使用的训练集。这就包含了以下几件事：</p>
<ul>
<li>把所有数据变成数字</li>
<li>补足空白的值</li>
<li>给每一行赋予一个<code>performance_count</code>和一个<code>foreclosure_status</code></li>
<li>删除那些没有表现历史数据的行（那些<code>performance_count</code>很低的行）</li>
</ul>
<p>我们有几列的数据都是文字，这在机器学习里没有什么用。然而它们其实是类别变量，比如说<code>R</code>,<code>S</code>这样的类别编号。我们分别赋予它们数字，从而把它们变成数字：</p>
<p><img src="https://www.dropbox.com/s/cy5mueqz7vgxxa2/Screenshot%202016-09-29%2022.40.02.png?raw=1" alt=""></p>
<p>这样转化了它们之后就能把它们在机器学习上用到。</p>
<p>一些列也包含了时间（<code>first_payment_date</code>和<code>origination_date</code>）。我们可以分别把它们分割成两列：</p>
<p><img src="https://www.dropbox.com/s/fumpjfvbim1ot9d/Screenshot%202016-09-29%2022.41.22.png?raw=1" alt=""></p>
<p>下面的代码里，我们会转换收购数据。我们会定义一个函数，这个函数会：</p>
<ul>
<li>通过从<code>counts</code>字典里获取数据，在<code>acquisition</code>里建立一个<code>foreclosure_status</code>列</li>
<li>通过从<code>counts</code>字典里获取数据，在<code>acquisition</code>里建立一个<code>performance_count</code>列</li>
<li>把下面的列从文字转成数字：<ul>
<li><code>channel</code> </li>
<li><code>seller</code></li>
<li><code>first_time_homebuyer</code></li>
<li><code>loan_purpose</code></li>
<li><code>property_type</code></li>
<li><code>occupancy_status</code></li>
<li><code>property_state</code></li>
<li><code>product_type</code></li>
</ul>
</li>
<li>分别把<code>first_payment_date</code>和<code>origination_date</code>转换成两列：<ul>
<li>遇见<code>/</code>就分割</li>
<li>把第一部分赋予<code>month</code>列</li>
<li>把第二部分赋予<code>year</code>列</li>
<li>删除原本列</li>
<li>最后，我们就会有<code>first_payment_month</code>, <code>first_payment_year</code>, <code>origination_month</code>和<code>origination_year</code></li>
</ul>
</li>
<li>在<code>acquisition</code>里的所有缺失值都替换成<code>-1</code></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">def annotate(acquisition, counts):</div><div class="line">    acquisition[&quot;foreclosure_status&quot;] = acquisition[&quot;id&quot;].apply(lambda x: get_performance_summary_value(x, &quot;foreclosure_status&quot;, counts))</div><div class="line">    acquisition[&quot;performance_count&quot;] = acquisition[&quot;id&quot;].apply(lambda x: get_performance_summary_value(x, &quot;performance_count&quot;, counts))</div><div class="line">    for column in [</div><div class="line">        &quot;channel&quot;,</div><div class="line">        &quot;seller&quot;,</div><div class="line">        &quot;first_time_homebuyer&quot;,</div><div class="line">        &quot;loan_purpose&quot;,</div><div class="line">        &quot;property_type&quot;,</div><div class="line">        &quot;occupancy_status&quot;,</div><div class="line">        &quot;property_state&quot;,</div><div class="line">        &quot;product_type&quot;</div><div class="line">    ]:</div><div class="line">        acquisition[column] = acquisition[column].astype(&apos;category&apos;).cat.codes</div><div class="line"></div><div class="line">    for start in [&quot;first_payment&quot;, &quot;origination&quot;]:</div><div class="line">        column = &quot;&#123;&#125;_date&quot;.format(start)</div><div class="line">        acquisition[&quot;&#123;&#125;_year&quot;.format(start)] = pd.to_numeric(acquisition[column].str.split(&apos;/&apos;).str.get(1))</div><div class="line">        acquisition[&quot;&#123;&#125;_month&quot;.format(start)] = pd.to_numeric(acquisition[column].str.split(&apos;/&apos;).str.get(0))</div><div class="line">        del acquisition[column]</div><div class="line"></div><div class="line">    acquisition = acquisition.fillna(-1)</div><div class="line">    acquisition = acquisition[acquisition[&quot;performance_count&quot;] &gt; settings.MINIMUM_TRACKING_QUARTERS]</div><div class="line">    return acquisition</div></pre></td></tr></table></figure>
<p>#连接所有的结果</p>
<p>我们就快准备好把所有结果联系起来了，在这之前我们只是要再加一些代码到<code>annotate.py</code>里。在下面的代码中，我们：</p>
<ul>
<li>定义一个函数来读取收购数据</li>
<li>顶一个函数把处理过的数据写入<code>processed/train.csv</code></li>
<li>如果文件是从命令行传入的，比如<code>python annotate.py</code>，就:<ul>
<li>读取收购数据</li>
<li>计算表现数据的累计数目，并赋予<code>counts</code></li>
<li>给<code>acquisition</code>DataFrame做标记</li>
<li>把<code>acquisition</code>DataFrame写入<code>train.csv</code></li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">def read():</div><div class="line">    acquisition = pd.read_csv(os.path.join(settings.PROCESSED_DIR, &quot;Acquisition.txt&quot;), sep=&quot;|&quot;)</div><div class="line">    return acquisition</div><div class="line">    </div><div class="line">def write(acquisition):</div><div class="line">    acquisition.to_csv(os.path.join(settings.PROCESSED_DIR, &quot;train.csv&quot;), index=False)</div><div class="line"></div><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">    acquisition = read()</div><div class="line">    counts = count_performance_rows()</div><div class="line">    acquisition = annotate(acquisition, counts)</div><div class="line">    write(acquisition)</div></pre></td></tr></table></figure>
<p>写好文件后，记得用<code>python annotate.py</code>来运行它，得到一个<code>train.csv</code>文件。完整的<code>annotate.py</code>文件在<a href="https://github.com/dataquestio/loan-prediction/blob/master/annotate.py" target="_blank" rel="external">这里</a></p>
<p>文件夹现在应该长这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">loan-prediction</div><div class="line">├── data</div><div class="line">│   ├── Acquisition_2012Q1.txt</div><div class="line">│   ├── Acquisition_2012Q2.txt</div><div class="line">│   ├── Performance_2012Q1.txt</div><div class="line">│   ├── Performance_2012Q2.txt</div><div class="line">│   └── ...</div><div class="line">├── processed</div><div class="line">│   ├── Acquisition.txt</div><div class="line">│   ├── Performance.txt</div><div class="line">│   ├── train.csv</div><div class="line">├── .gitignore</div><div class="line">├── annotate.py</div><div class="line">├── assemble.py</div><div class="line">├── README.md</div><div class="line">├── requirements.txt</div><div class="line">├── settings.py</div></pre></td></tr></table></figure>
<h1 id="找到误差衡量方法"><a href="#找到误差衡量方法" class="headerlink" title="找到误差衡量方法"></a>找到误差衡量方法</h1><p>我们已经完成如何生成数据了，现在我们只需要完成最后一步，生成预测。我们需要弄明白一个误差的衡量方法，以及我们如何衡量我们的数据。这里来说，没有被止赎的贷款比止赎的贷款多得多，所以典型的准确度不太有用。</p>
<p>如果我们看一看训练数据，并看一看<code>foreclosure_status</code>列的计数，我们会得到这些：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">import settings</div><div class="line"></div><div class="line">train = pd.read_csv(os.path.join(settings.PROCESSED_DIR, &quot;train.csv&quot;))</div><div class="line">train[&quot;foreclosure_status&quot;].value_counts()</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">False    4635982</div><div class="line">True        1585</div><div class="line">Name: foreclosure_status, dtype: int64</div></pre></td></tr></table></figure>
<p>因为只有这么一点点贷款是止赎了，所以如果我们只看有多少百分比的标签被正确预测了，那我们就可以建立一个只预测<code>False</code>的模型，一样可以得到很高的准确度。所以我们会用一个把这种不平衡考虑进去，保证我们准确预测止赎的误差测量法。我们不像要太多假的正预测，也就是我们预测一个贷款会止赎，但其实不会，或者太多的假的负样本，也就是我们预测一个贷款不会被止赎，但其实会。在这两者之间，假的负样本对Fannie Mae来说消耗更大，因为Fannie Mae就会买一些不能够弥补他们的投资的房贷了。</p>
<p>我们定义假的负样本率为我们预测不会止赎但其实会的预测数量，除以总的止赎贷款数量。这就是模型错过的止赎的百分比。下面是一个图表：</p>
<p><img src="https://www.dropbox.com/s/vauljb4xp2qnz7n/Screenshot%202016-09-29%2023.16.57.png?raw=1" alt=""></p>
<p>在上图表中，<code>1</code>个贷款被预测为非止赎，但它其实是。如果我们把它除以真的止赎的贷款数量,<code>2</code>，我们就得到的假的负样本率,<code>50%</code>。我们会用它作为我们的误差衡量，这样我们能够有效地评估我们模型的表现。</p>
<p>#为机器学习设置好分类器</p>
<p>我们会使用交叉验证来做预测。用交叉验证，我们会把数据分成<code>3</code>组，然后做以下的事：</p>
<ul>
<li>在<code>1</code>组和<code>2</code>组上训练，然后在<code>3</code>组上预测</li>
<li>在<code>1</code>组和<code>3</code>组上训练，然后在<code>2</code>组上预测</li>
<li>在<code>2</code>组和<code>3</code>组上训练，然后在<code>1</code>组上预测</li>
</ul>
<p>把它分成几组意味着我们不会用同样的数据来训练模型然后又用同样的数据来做预测。这就避免了过拟合。如果我们过拟合了，我们就会得到一个假的很低的假的负样本率，也就是说我们的模型很难在真实世界应用。</p>
<p><a href="http://scikit-learn.org/" target="_blank" rel="external">Scikit-learn</a>有一个叫做<a href="http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_predict.html" target="_blank" rel="external">cross_val_predict</a>的函数，它让我们做交叉验证变得很容易。</p>
<p>我们还需要挑选一个算法来做预测。我们需要一个分类器来做<a href="https://en.wikipedia.org/wiki/Binary_classification" target="_blank" rel="external">二元分类</a>。因为目标变量，<code>foreclosure_status</code>只有两个值，<code>True</code>和<code>Flase</code></p>
<p>我们会使用<a href="https://en.wikipedia.org/wiki/Logistic_regression" target="_blank" rel="external">logistic regression</a>。因为它在二元分类下表现很好 – 运行得极快,而且消耗很少内存.这是因为这个算法的工作方式 – 它比起建立一堆决策树,比如随机森林,或者做一些很昂贵的计算,比如支持向量机来说,需要少得多的步骤,只需要一些矩阵的操作而已.</p>
<p>我们可以用scikit-learn里自带的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" target="_blank" rel="external">logistic regression classifier</a>算法. 我们需要注意的唯一一件事就是每个类的weight. 如果我们给每个类同样的比重, 算法就会给每一行预测<code>False</code>, 因为它想要最小化误差. 然而,我们更关心止赎的贷款而不是不会止赎的贷款. 因此,我们给<a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" target="_blank" rel="external">Logistic Regression</a>传入<code>balanced</code>参数到<code>class_weight</code>关键词里, 从而得到一个考虑样本数量而给于平衡的比重的算法.这样就能确保算法不会对每一行都预测<code>False</code>, 而是在出现误差时对每个类都给予相等的惩罚.</p>
<p>#做预测</p>
<p>既然我们所有的前提都搞定了, 我们可以开始做预测了. 我们会创建一个新文件叫做<code>predict.py</code>, 它会使用我们之前创建的<code>train.csv</code>. 下面的代码会:</p>
<ul>
<li>导入需要的库</li>
<li>创建一个<code>cross_validate</code>函数,它会:<ul>
<li>用正确的关键词参数创建一个logitic regression分类器</li>
<li>创建我们想用来训练模型的数据列, 同时删除<code>id</code>he <code>foreclosure_statsu</code></li>
<li>在<code>train</code>DataFrame上运行交叉验证</li>
<li>返回预测</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">import settings</div><div class="line">import pandas as pd</div><div class="line">from sklearn import cross_validation</div><div class="line">from sklearn.linear_model import LogisticRegression</div><div class="line">from sklearn import metrics</div><div class="line"></div><div class="line">def cross_validate(train):</div><div class="line">    clf = LogisticRegression(random_state=1, class_weight=&quot;balanced&quot;)</div><div class="line"></div><div class="line">    predictors = train.columns.tolist()</div><div class="line">    predictors = [p for p in predictors if p not in settings.NON_PREDICTORS]</div><div class="line"></div><div class="line">    predictions = cross_validation.cross_val_predict(clf, train[predictors], train[settings.TARGET], cv=settings.CV_FOLDS)</div><div class="line">    return predictions</div></pre></td></tr></table></figure>
<p>#预测误差</p>
<p>现在我们只需要写一些函数来计算误差.下面的代码会:</p>
<ul>
<li>创建一个函数<code>computer_error</code>, 它会:<ul>
<li>用scikit-learn计算一个简单准确度评分(符合真实<code>foreclosure_status</code>值的预测的百分比)</li>
</ul>
</li>
<li>创建一个函数<code>computer_false_negatives</code>, 它会:<ul>
<li>把目标和预测写进一个DataFrame</li>
<li>计算假的负样本率</li>
</ul>
</li>
<li>创建一个函数<code>computer_false_positives</code>, 它会:<ul>
<li>把目标和预测写进一个DataFrame</li>
<li>计算假的正样本率<ul>
<li>找到模型预测为止赎但并不是的贷款的数量</li>
<li>用这个数量除以不是止赎的贷款数量</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def compute_error(target, predictions):</div><div class="line">    return metrics.accuracy_score(target, predictions)</div><div class="line"></div><div class="line">def compute_false_negatives(target, predictions):</div><div class="line">    df = pd.DataFrame(&#123;&quot;target&quot;: target, &quot;predictions&quot;: predictions&#125;)</div><div class="line">    return df[(df[&quot;target&quot;] == 1) &amp; (df[&quot;predictions&quot;] == 0)].shape[0] / (df[(df[&quot;target&quot;] == 1)].shape[0] + 1)</div><div class="line"></div><div class="line">def compute_false_positives(target, predictions):</div><div class="line">    df = pd.DataFrame(&#123;&quot;target&quot;: target, &quot;predictions&quot;: predictions&#125;)</div><div class="line">    return df[(df[&quot;target&quot;] == 0) &amp; (df[&quot;predictions&quot;] == 1)].shape[0] / (df[(df[&quot;target&quot;] == 0)].shape[0] + 1)</div></pre></td></tr></table></figure>
<p>#把所有东西连在一起</p>
<p>现在,我们已经把函数都放在了<code>predict.py</code>里面.下面的代码会:</p>
<ul>
<li>读取数据集</li>
<li>计算交叉验证预测</li>
<li>计算上述<code>3</code>个误差值</li>
<li>打印出误差值</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">def read():</div><div class="line">    train = pd.read_csv(os.path.join(settings.PROCESSED_DIR, &quot;train.csv&quot;))</div><div class="line">    return train</div><div class="line">    </div><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">    train = read()</div><div class="line">    predictions = cross_validate(train)</div><div class="line">    error = compute_error(train[settings.TARGET], predictions)</div><div class="line">    fn = compute_false_negatives(train[settings.TARGET], predictions)</div><div class="line">    fp = compute_false_positives(train[settings.TARGET], predictions)</div><div class="line">    print(&quot;Accuracy Score: &#123;&#125;&quot;.format(error))</div><div class="line">    print(&quot;False Negatives: &#123;&#125;&quot;.format(fn))</div><div class="line">    print(&quot;False Positives: &#123;&#125;&quot;.format(fp))</div></pre></td></tr></table></figure>
<p>一旦你加上了这些代码,你可以运行<code>python predict.py</code>来生成预测. 运行上面的代码给予我们假的负样本率为<code>.26</code>,也就是说对于止赎贷款来说,我们错误地预测了<code>26%</code>.这是个好的开始,但我们还可以做很多提升!</p>
<p>完整的<code>predict.py</code>文件在<a href="https://github.com/dataquestio/loan-prediction/blob/master/predict.py" target="_blank" rel="external">这里</a></p>
<p>你现在的文件树应该长这样:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">loan-prediction</div><div class="line">├── data</div><div class="line">│   ├── Acquisition_2012Q1.txt</div><div class="line">│   ├── Acquisition_2012Q2.txt</div><div class="line">│   ├── Performance_2012Q1.txt</div><div class="line">│   ├── Performance_2012Q2.txt</div><div class="line">│   └── ...</div><div class="line">├── processed</div><div class="line">│   ├── Acquisition.txt</div><div class="line">│   ├── Performance.txt</div><div class="line">│   ├── train.csv</div><div class="line">├── .gitignore</div><div class="line">├── annotate.py</div><div class="line">├── assemble.py</div><div class="line">├── predict.py</div><div class="line">├── README.md</div><div class="line">├── requirements.txt</div><div class="line">├── settings.py</div></pre></td></tr></table></figure>
<h1 id="写一个README"><a href="#写一个README" class="headerlink" title="写一个README"></a>写一个README</h1><p>既然我们已经完成了我们的项目, 我们只需要写一个<code>README.md</code>文件,让其他人知道我们做了什么,以及如何复制它.一个典型的<code>README.md</code>应该包括以下内容:</p>
<ul>
<li>一个高屋建瓴的项目概览,以及目标是什么</li>
<li>哪里去下载所需的数据和材料</li>
<li>安装教程<ul>
<li>如何安装需要的模块</li>
</ul>
</li>
<li>使用教程<ul>
<li>如何运行项目</li>
<li>每一步应该看到哪些结果</li>
</ul>
</li>
<li>如何向这个项目做贡献<ul>
<li>扩展这个项目的好的下一步</li>
</ul>
</li>
</ul>
<p><a href="https://github.com/dataquestio/loan-prediction/blob/master/README.md" target="_blank" rel="external">这里</a>是本项目的一个样本<code>README.md</code></p>
<h1 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h1><p>恭喜,你已经完成了一个从头到文的机器学习项目!你可在<a href="https://github.com/dataquestio/loan-prediction" target="_blank" rel="external">这里</a>找到完整的样本项目.你完成项目之后,上传到Github上会是个好主意,这样其他人就会看到这是你作品集的一部分.</p>
<p>这些数据尚有一些地方待你挖掘.大致来说,我们可以把它们分成<code>3</code>类 – 延展项目,把它做得更准确, 找到其他数据来预测, 探索数据. 这里有一些想法:</p>
<ul>
<li>用<code>annotate.py</code>生成更多特征</li>
<li>在<code>predict.py</code>里换个算法</li>
<li>使用Fannie Mae里的更多数据</li>
<li>加上一个预测未来数据的方法.现在我们写的代码在我们加上更多数据时也会运行,所以我们可以加上更多过去的或者未来的数据</li>
<li>尝试能不能预测银行一开始该不该放出贷款(vs Fannie Mae应不应该收购贷款)<ul>
<li>删除那些银行在发放贷款时不能获得的信息<ul>
<li>一些列是在Fannie Mae收购的时候有的,但之前没有</li>
</ul>
</li>
<li>做预测</li>
</ul>
</li>
<li>探索一下看你能不能预测除了<code>foreclosure_status</code>以外的数据<ul>
<li>你能不能预测房产在发售的时候卖多少钱?</li>
</ul>
</li>
<li>探索一下表现数据更新时的细节<ul>
<li>你能不能预测借方迟付贷款的次数?</li>
<li>你能不能画出典型的贷款周期?</li>
</ul>
</li>
<li>把数据按州分类,或者按邮政编码分类<ul>
<li>你有看到一些有趣的模式吗?</li>
</ul>
</li>
</ul>
<p>如果你建立了一些有趣的项目, 请在留言区让我们知道!</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/09/21/data-sceince-portfolio-machine-learning-project/" data-id="ciuy2tjkb0004ysayvmd4s2dt" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-the_annoying_jupyter_notebook" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/09/18/the_annoying_jupyter_notebook/" class="article-date">
  <time datetime="2016-09-18T15:05:00.000Z" itemprop="datePublished">2016-09-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/18/the_annoying_jupyter_notebook/">jupyter notebook调用不同environment解决方案</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>如果你使用Anaconda作为你的Python管理系统，而且如果你有不同的conda environment，那么接下来我讲的事情你可能会有同感。</p>
<p>我打算在一个新的conda环境里安装tensorflow，我是这么做的：<br>(Mac OS X, GPU enabled, Python 2.7)</p>
<p>用下面的代码创建一个新的环境：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ conda env create -n tensorflow python=2.7</div></pre></td></tr></table></figure></p>
<p>之后activate它：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ source activate tensorflow</div></pre></td></tr></table></figure></p>
<p>接着安装tensorflow:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.10.0-py2-none-any.whl</div><div class="line">(tensorflow)$ pip install --ignore-installed --upgrade $TF_BINARY_URL</div></pre></td></tr></table></figure></p>
<p>成功之后，我很高兴，所以我二话不说就打开了jupyter notebook:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(tensorflow)$ jupyter notebook</div></pre></td></tr></table></figure></p>
<p>结果，系统报错： <strong>No module named tensorflow</strong><br>于是我退出，用<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(tensorflow)$ which jupyter</div></pre></td></tr></table></figure>查看了一下我正在使用的jupyter, 结果令人失望：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/Users/jasontang/anaconda/bin/jupyter</div></pre></td></tr></table></figure></p>
<p>根本没有使用tensorflow env里的jupyter，怪不得报错！</p>
<p>可是我明明转换了环境呀，如果转换环境不能转换executable的路径的话，转换环境又有什么用呢？</p>
<h3>解决方法</h3>

<p>首先运行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python -m IPython kernelspec install-self</div></pre></td></tr></table></figure>
<p>然后：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vi /usr/local/share/jupyter/kernels/python2/kernel.json</div></pre></td></tr></table></figure>
<p>将 </p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line"> <span class="attr">"display_name"</span>: <span class="string">"Python 2"</span>,</div><div class="line"> <span class="attr">"language"</span>: <span class="string">"python"</span>,   </div><div class="line"> <span class="attr">"argv"</span>: [</div><div class="line">  <span class="string">"/Users/yourname/anaconda/bin/python"</span>,  &lt;-- Change this line</div><div class="line">  <span class="string">"-m"</span>,</div><div class="line">  <span class="string">"ipykernel"</span>,</div><div class="line">  <span class="string">"-f"</span>,</div><div class="line">  <span class="string">"&#123;connection_file&#125;"</span></div><div class="line"> ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>改成<br><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line"> <span class="attr">"display_name"</span>: <span class="string">"Python 2"</span>,</div><div class="line"> <span class="attr">"language"</span>: <span class="string">"python"</span>,   </div><div class="line"> <span class="attr">"argv"</span>: [</div><div class="line">  <span class="string">"python"</span>,  &lt;-- Change this line</div><div class="line">  <span class="string">"-m"</span>,</div><div class="line">  <span class="string">"ipykernel"</span>,</div><div class="line">  <span class="string">"-f"</span>,</div><div class="line">  <span class="string">"&#123;connection_file&#125;"</span></div><div class="line"> ]</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3>为什么</h3>

<p>首先，要怪的话就怪jupyter的设计。jupyter使用的kernel是根据kernel.json里定义的python路径来选择，而当我们使用source activate tensorflow的时候，这份kernel.json并没有被改变。<br>当我们将”/Users/yourname/anaconda/bin/python”改成”python”时， 系统会自动去PATH里寻找默认的python executable，这时它会找到环境里的python。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/09/18/the_annoying_jupyter_notebook/" data-id="ciuy2tjkl0006ysayb5s6lp9x" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Jupyter-Notebook-Environment-Anaconda/">Jupyter Notebook Environment Anaconda</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-SAME_is_not_same_in_tf" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/09/18/SAME_is_not_same_in_tf/" class="article-date">
  <time datetime="2016-09-18T03:40:00.000Z" itemprop="datePublished">2016-09-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/18/SAME_is_not_same_in_tf/">tf.nn.conv2d里的padding解析</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Tensorflow的tf.nn.conv2d有个与其他库不一样的地方 – padding。<br>根据<a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">CS231n</a>的说法:<br>$$ out\_width = (W-F+2P)/S + 1 $$<br>W是图像宽度，F是filter宽度，P是padding, S是stride。</p>
<p>然而在Tensorflow里却不一样。</p>
<p>tf.nn.conv2d里的padding有两个选项： ‘SAME’ 和 ‘VALID’。 很多人认为’SAME’就是使得out_width和in_width一样，其实不是的。</p>
<p>根据官方的<a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#convolution" target="_blank" rel="external">计算公式</a>:</p>
<p>$$ out\_width = ceil(float(in\_width) / float(strides[2])) $$</p>
<p>举个栗子， in_width = 28， strides = [1, 2, 2, 1] 那么：</p>
<p>$$ out\_width = ceil( 28. / 2. ) = 14 $$<br>并不是想当然的28哦。</p>
<p>那么问题来了，为什么要给这个选项取名’SAME’呢？</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/09/18/SAME_is_not_same_in_tf/" data-id="ciuy2tjk60002ysay6pa3tq9c" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-macOS_Sierra_on_PC" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/09/17/macOS_Sierra_on_PC/" class="article-date">
  <time datetime="2016-09-16T16:00:00.000Z" itemprop="datePublished">2016-09-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/17/macOS_Sierra_on_PC/">在PC上安装macOS Sierra</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>我花了两天时间，断断续续地在一台PC上安装了最新的macOS Sierra：<br><img src="https://www.dropbox.com/s/cqt6uq3y2swkkx4/Screenshot%202016-09-17%2017.13.20.png?raw=1" alt=""></p>
<p>安装的过程无比艰辛。</p>
<p>我主要参考了两个Post， <a href="http://www.tonymacx86.com/threads/unibeast-install-os-x-el-capitan-on-any-supported-intel-based-pc.172672/#download_elcap" target="_blank" rel="external">TonyMacx86</a> 和 <a href="https://www.youtube.com/watch?v=Cw0O_cesGJo" target="_blank" rel="external">Cat and Andrew</a> （Youtube, 推荐这个），稍后我会把视频上传到国内网站上。</p>
<p>之所以想安装黑苹果，是因为我不想浪费了自己的GTX970。如果你和我一样需要用到GTX 700或更新的GPU，就需要注意下面这几点：</p>
<p>— 按Cat and Andrew步骤安装好之后， 找到电脑的Model Number, 如图：<br><img src="https://www.dropbox.com/s/dsgco1hotkwh4y0/Screenshot%202016-09-17%2017.28.03.png?raw=1" alt=""></p>
<ul>
<li><p>按着这个Model Number，去<a href="http://www.insanelymac.com/forum/topic/312525-nvidia-web-driver-updates-for-macos-sierra-update-08292016/page-1" target="_blank" rel="external">这里</a>下载对应的Nvidia Web Driver</p>
</li>
<li><p>安装Web Driver</p>
</li>
<li><p>重启， 添加boot arg: “nv_disable=1” (不含引号)</p>
</li>
<li><p>改变Driver， 使用Nvidia Web Driver， 如图:<br><img src="https://www.dropbox.com/s/38suagcgfyohhcv/Screenshot%202016-09-17%2017.31.09.png?raw=1" alt=""></p>
</li>
<li><p>重启， 添加boot arg: “nvda_drv=1” </p>
</li>
<li><p>现在系统应该可以使用Nvidia Web Driver了， 看一看：<br><img src="https://www.dropbox.com/s/00nckirrhbv75gv/Screenshot%202016-09-17%2017.33.47.png?raw=1" alt=""> 哎呀不是？<br>点一下就可以了。<br><img src="https://www.dropbox.com/s/1k28a24otdiq1nh/Screenshot%202016-09-17%2017.34.29.png?raw=1" alt=""></p>
</li>
</ul>
<p>这样就装好了一台使用NVIDIA Web Driver的Mac！这样做的好处在于，你可以任意安装自己喜欢的GPU（980, Titan, 1080 etc.）</p>
<p>因为就算几万块的Mac Pro 也不会配备高端的GPU（扼腕， 这真的是苹果高端电脑的弊端），所以你的黑苹果可以算是苹果机中的战斗机了（在你有好配置的前提下）。</p>
<p><img src="http://cdn2.macworld.co.uk/cmsdata/features/3536364/Mac-Pro-Front-Back.jpg" alt=""></p>
<p>还在等什么？赶紧把你家的PC备份，装个战斗机！</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/09/17/macOS_Sierra_on_PC/" data-id="ciuy2tjk90003ysaybs8g0jbv" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Machine-Learning-Scholar-Rank" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/08/15/Machine-Learning-Scholar-Rank/" class="article-date">
  <time datetime="2016-08-14T16:11:42.000Z" itemprop="datePublished">2016-08-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/08/15/Machine-Learning-Scholar-Rank/">机器学习论文的统计</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这是一篇统计IEEE上关于机器学习论文的文章。<br>我是一个机器学习的新手，一开始我找不到应该读哪些论文，于是我想，干嘛不用python统计一下哪些论文最重要呢？<br>于是有了这篇文章。</p>
<p>第一步，是获得数据。这一次我选择了IEEE Xplore作为数据来源。这是它的网站界面：</p>
<p><img src="https://www.dropbox.com/s/rcmfmshb8vpzk91/Screenshot%202016-08-15%2009.52.50.png?raw=1" alt=""></p>
<p>让我们做一个简单的搜索，看看它的网站有什么变化：</p>
<p><img src="https://www.dropbox.com/s/kjdzsqjf5jg8qkm/Screenshot%202016-08-15%2009.54.30.png?raw=1" alt=""></p>
<p>我们发现搜索结果看起来是有结构的，很好！让我们再来看看它的HTML结构：</p>
<p><img src="https://www.dropbox.com/s/pyhiek8rb8udsfo/Screenshot%202016-08-15%2009.57.08.png?raw=1" alt=""></p>
<p>我们发现每一个搜索结果都被包裹在一个div里面，这个div的类是List-results-items。我们还可以看一眼div里面的结构，发现我们关心的几个数据：<strong>标题</strong>，<strong>年份</strong>， <strong>作者</strong>，<strong>引用次数</strong>，<strong>期刊</strong>， <strong>摘要</strong>。</p>
<p>我打算写一个python的爬虫，先把网页的源代码扒下来，然后再对源代码进行解析，最后通过不同的HTML Tag取得我们想要的数据。</p>
<p>我一开始尝试使用urllib2进行访问，但获取到的源代码里并没有我想要的东西，于是我意识到这是一个动态网页，我们需要用一些动态网页的包进行访问。什么是动态网页呢? 简单的说，就是部分（或全部）内容是由Javascript生成的。这也解释了为什么一开始用urllib2进行访问不成功的原因，因为urllib2的请求并没有执行JS的能力，所以我们要的论文信息就不会被捕捉到。</p>
<p>讲了这么多，实际有一个叫做<a href="http://www.seleniumhq.org/" target="_blank" rel="external">selenium</a>的包就能做动态网页的请求。简单地说，selenium是一个为自动化网站测试而开发的程序，它几乎可以在各个方面模拟一个浏览器的行为。Selenium还有一个<a href="http://selenium-python.readthedocs.io/" target="_blank" rel="external">python的封包</a>，正是我想要的。</p>
<p>安装selenium很简单， 用pip的话：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install selenium</div></pre></td></tr></table></figure>
<p>用conda的话：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conda install -c metaperl selenium=2.40.0</div></pre></td></tr></table></figure>
<p>安装好了之后，打开你最喜欢的python编辑器<br>下面是一段简单的selenium 代码, 打开一个Firefox浏览器的实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"></div><div class="line">driver = webdriver.Firefox()</div></pre></td></tr></table></figure>
<p>一个新的Firefox浏览器应该会弹出来。</p>
<p>之后，我拟定了四个主题：’Machine Learning’, ‘Deep Learning’, ‘Data Mining’, ‘Neural Network’。我准备扒下IEEE Xplore里面关于这四个主题的所有paper信息。</p>
<p>下面是我的准备工作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">topics = &#123;&#125;</div><div class="line">ml = &#123;</div><div class="line">    <span class="string">'keyword'</span> : <span class="string">'machine%20learning'</span>,</div><div class="line">    <span class="string">'page_count'</span> : <span class="number">508</span>,</div><div class="line">    <span class="string">'total'</span>: <span class="number">50800</span>,</div><div class="line">&#125;</div><div class="line">dl = &#123;</div><div class="line">    <span class="string">'keyword'</span> : <span class="string">'deep%20learning'</span>,</div><div class="line">    <span class="string">'page_count'</span> : <span class="number">29</span>,</div><div class="line">    <span class="string">'total'</span> : <span class="number">2900</span>,</div><div class="line">&#125;</div><div class="line">nn = &#123;</div><div class="line">    <span class="string">'keyword'</span> : <span class="string">'neural%20network'</span>,</div><div class="line">    <span class="string">'page_count'</span> : <span class="number">1221</span>,</div><div class="line">    <span class="string">'total'</span> : <span class="number">122100</span>,</div><div class="line">&#125;</div><div class="line">dm = &#123;</div><div class="line">    <span class="string">'keyword'</span> : <span class="string">'data%20mining'</span>,</div><div class="line">    <span class="string">'page_count'</span> : <span class="number">847</span>,</div><div class="line">    <span class="string">'total'</span> : <span class="number">84700</span></div><div class="line">&#125;</div><div class="line">topics[<span class="string">'ml'</span>] = ml</div><div class="line">topics[<span class="string">'dl'</span>] = dl</div><div class="line">topics[<span class="string">'nn'</span>] = nn</div><div class="line">topics[<span class="string">'dm'</span>] = dm</div></pre></td></tr></table></figure>
<p>上面的数字，比如<code>&#39;page_count&#39;: 508</code>, 是在每一页100份论文的前提下，IEEE XPlore的最大页数。</p>
<p>准备工作完成，我们可以开始抓取source了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 抓取source 分步</span></div><div class="line">soups = []</div><div class="line"></div><div class="line"><span class="keyword">for</span> topic <span class="keyword">in</span> topics:</div><div class="line">    print(<span class="string">'Topic: %s'</span>,topics[topic][<span class="string">'keyword'</span>])</div><div class="line"></div><div class="line">    length = topics[topic][<span class="string">'page_count'</span>]</div><div class="line">    pbar = ProgressBar(length)  <span class="comment"># 显示进度</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>, topics[topic][<span class="string">'page_count'</span>]):        </div><div class="line">        driver.get(<span class="string">'http://ieeexplore.ieee.org/search/searchresult.jsp?newsearch=true&amp;queryText='</span>+ topics[topic][<span class="string">'keyword'</span>] +<span class="string">'&amp;rowsPerPage=100&amp;pageNumber='</span>+str(i))</div><div class="line"></div><div class="line">        time.sleep(<span class="number">5</span>)</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(<span class="number">5</span>):  <span class="comment"># 重要： 因为网页只有下拉之后才会加载，所以这里连续下拉5次，每次留给服务器1秒钟的加载时间</span></div><div class="line">            driver.execute_script(<span class="string">"window.scrollTo(0, "</span>+ str(<span class="number">100000</span>) +<span class="string">");"</span>)</div><div class="line">            driver.execute_script(<span class="string">"window.scrollTo(0, document.body.scrollHeight);"</span>)</div><div class="line">            time.sleep(<span class="number">2</span>)</div><div class="line">        source = driver.page_source</div><div class="line">        soup = bs(source)</div><div class="line">        soups.append(soup)</div><div class="line"></div><div class="line">        pbar.increment()</div><div class="line"></div><div class="line">    pbar.finish()</div></pre></td></tr></table></figure>
<p>事实证明，动态网页的抓取非常耗时，这里4个topic分别花了我:</p>
<table>
<thead>
<tr>
<th>主题</th>
<th style="text-align:center">页数</th>
<th style="text-align:right">时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>Machine Learning</td>
<td style="text-align:center">508</td>
<td style="text-align:right">7.29 hr</td>
</tr>
<tr>
<td>Deep Learning</td>
<td style="text-align:center">29</td>
<td style="text-align:right">0.58 hr</td>
</tr>
<tr>
<td>Data Mining</td>
<td style="text-align:center">847</td>
<td style="text-align:right">10.72 hr</td>
</tr>
<tr>
<td>Neural Network</td>
<td style="text-align:center">1221</td>
<td style="text-align:right">9.74 hr</td>
</tr>
</tbody>
</table>
<p>也就是两天。</p>
<p>不过，拿到的数据量还是非常可喜的，各个主题的源代码加在一起有1.33G左右。</p>
<p>下面，就是分析的工作。我从源代码中发现的数据规律可以在这里应用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Parsing</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> unicodecsv <span class="keyword">as</span> csv</div><div class="line"></div><div class="line">years = []</div><div class="line">titles = []</div><div class="line">authors = []</div><div class="line">publishers = []</div><div class="line">abstracts = []</div><div class="line">citeds = []</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">innerHTML</span><span class="params">(element)</span>:</span>  <span class="comment"># 获取节点中的内容</span></div><div class="line">    <span class="keyword">return</span> element.decode_contents(formatter=<span class="string">"html"</span>)</div><div class="line"></div><div class="line"></div><div class="line">base = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> topic <span class="keyword">in</span> topics:</div><div class="line">    <span class="keyword">for</span> soup <span class="keyword">in</span> soups[base:base+topics[topic][<span class="string">'page_count'</span>]]:</div><div class="line"></div><div class="line">        results = soup.select(<span class="string">'div.List-results-items'</span>)</div><div class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> results:</div><div class="line">            title = r.select(<span class="string">'h2 a.ng-binding'</span>)</div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> title:</div><div class="line">                title = r.select(<span class="string">'h2 span'</span>)</div><div class="line">            year = r.select(<span class="string">'span[ng-if="::record.publicationYear"]'</span>)</div><div class="line">            author = r.select(<span class="string">'span[ng-bind-html="::author.preferredName"]'</span>)</div><div class="line">            publisher = r.select(<span class="string">'a[ng-bind-html="::record.publicationTitle"]'</span>)</div><div class="line">            abstract = r.select(<span class="string">'span[ng-bind-html="::record.abstract"]'</span>)</div><div class="line">            cited = r.select(<span class="string">'span[ng-if="::record.citationCount"]'</span>)</div><div class="line"></div><div class="line"></div><div class="line">            title, n = re.subn(<span class="string">"\\&lt;.*?\\&gt;"</span>, <span class="string">''</span>, innerHTML(title[<span class="number">0</span>]))</div><div class="line"></div><div class="line">            year, n = re.subn(<span class="string">"\\&lt;.*?\\&gt;"</span>, <span class="string">''</span>, innerHTML(year[<span class="number">0</span>]))</div><div class="line">            year, n = re.subn(<span class="string">'\s*?'</span>, <span class="string">''</span>, year) <span class="comment"># 去掉无用的空格</span></div><div class="line">            year, n = re.subn(<span class="string">'Year:'</span>, <span class="string">''</span>, year) <span class="comment"># 去电'Year:'字段</span></div><div class="line"></div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> author:</div><div class="line">                author = <span class="string">u'No author'</span>  <span class="comment"># 可能出现没有作者的情况</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                author, n = re.subn(<span class="string">"\\&lt;.*?\\&gt;"</span>, <span class="string">''</span>, innerHTML(author[<span class="number">0</span>]))  <span class="comment"># 去掉内容里HTML标签的部分</span></div><div class="line"></div><div class="line">            publisher, n = re.subn(<span class="string">"\\&lt;.*?\\&gt;"</span>, <span class="string">''</span>, innerHTML(publisher[<span class="number">0</span>]))</div><div class="line"></div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> abstract:</div><div class="line">                abstract = <span class="string">u'No Abstract'</span></div><div class="line">            <span class="keyword">else</span>:                </div><div class="line">                abstract, n = re.subn(<span class="string">"\\&lt;.*?\\&gt;"</span>, <span class="string">''</span>, innerHTML(abstract[<span class="number">0</span>]))</div><div class="line"></div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cited:</div><div class="line">                cited = <span class="string">u'0'</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                cited, n = re.subn(<span class="string">"&lt;.*?&gt;"</span>, <span class="string">''</span>, innerHTML(cited[<span class="number">0</span>]))</div><div class="line">                cited = cited.replace(<span class="string">'Papers('</span>, <span class="string">''</span>) <span class="comment"># 去掉Papers(字段</span></div><div class="line">                cited = cited.replace(<span class="string">')'</span>, <span class="string">''</span>) <span class="comment"># 去掉）</span></div><div class="line"></div><div class="line">            <span class="comment"># 把它们存在csv里</span></div><div class="line">            <span class="keyword">with</span> open(topic+<span class="string">'.csv'</span>, <span class="string">'a'</span>) <span class="keyword">as</span> f:</div><div class="line">                writer = csv.writer(f, encoding=<span class="string">'utf8'</span>, delimiter=<span class="string">','</span>)</div><div class="line">                row = [title, author, year, cited, publisher, abstract]</div><div class="line">                writer.writerow(row)                </div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    base += topics[topic][<span class="string">'page_count'</span>]</div></pre></td></tr></table></figure>
<p>这样就得到了4个内容很整齐，去掉无关信息的csv文件。</p>
<p>长这样：<br><img src="https://www.dropbox.com/s/pv4jkdg14qafobt/Screenshot%202016-08-15%2014.55.39.png?raw=1" alt=""></p>
<p>值得一提的是，超过1.33G的源代码信息，经过我们的处理之后，每个csv文件只有20MB左右大小。<br>不由得感叹一句，大数据虽大，但是可能有用的数据其实就那么一点。</p>
<p>接下来，就是分析这些数据里隐藏的信息了。</p>
<p>我有6个目标：</p>
<ol>
<li>列出被引用最多的文章</li>
<li>列出被引用最多的作者</li>
<li>列出写最多文章的作者</li>
<li>列出被引用最多的出版商</li>
<li>标题里的词频分析</li>
<li>摘要里的词频分析</li>
</ol>
<p>为了快速地进行分析运算，我打算使用pandas包作为运算工具，matplotlib作为最后的画图工具，下面是我的准备工作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#-*- coding:utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">%matplotlib notebook  <span class="comment"># 为了在jupyter notebook 里inline 显示图像</span></div><div class="line"></div><div class="line">mldf = pd.read_csv(<span class="string">'csv/ml.csv'</span>, names=[<span class="string">'Title'</span>, <span class="string">'Author'</span>, <span class="string">'Year'</span>, <span class="string">'CitedCount'</span>, <span class="string">'Publisher'</span>, <span class="string">'Abstract'</span>])</div><div class="line">dmdf = pd.read_csv(<span class="string">'csv/dm.csv'</span>, names=[<span class="string">'Title'</span>, <span class="string">'Author'</span>, <span class="string">'Year'</span>, <span class="string">'CitedCount'</span>, <span class="string">'Publisher'</span>, <span class="string">'Abstract'</span>])</div><div class="line">dldf = pd.read_csv(<span class="string">'csv/dl.csv'</span>, names=[<span class="string">'Title'</span>, <span class="string">'Author'</span>, <span class="string">'Year'</span>, <span class="string">'CitedCount'</span>, <span class="string">'Publisher'</span>, <span class="string">'Abstract'</span>])</div><div class="line">nndf = pd.read_csv(<span class="string">'csv/nn.csv'</span> , names=[<span class="string">'Title'</span>, <span class="string">'Author'</span>, <span class="string">'Year'</span>, <span class="string">'CitedCount'</span>, <span class="string">'Publisher'</span>, <span class="string">'Abstract'</span>])</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_top</span><span class="params">(group)</span>:</span>  <span class="comment"># 把DataFrame根据CitedCount倒序排列</span></div><div class="line">    <span class="keyword">return</span> group.sort_index(by=<span class="string">'CitedCount'</span>, ascending=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">ml_top = get_top(mldf)</div><div class="line">dm_top = get_top(dmdf)</div><div class="line">dl_top = get_top(dldf)</div><div class="line">nn_top = get_top(nndf)</div><div class="line"></div><div class="line">dfs = &#123;  <span class="comment"># 创建一个DataFrame的字典， 以便后面的遍历</span></div><div class="line">    <span class="string">'ml'</span>: ml_top,</div><div class="line">    <span class="string">'dm'</span>: dm_top,</div><div class="line">    <span class="string">'dl'</span>: dl_top,</div><div class="line">    <span class="string">'nn'</span>: nn_top,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="画出最多引用的论文"><a href="#画出最多引用的论文" class="headerlink" title="画出最多引用的论文"></a>画出最多引用的论文</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">drawMostCitedPaper</span><span class="params">(CountList, TitleList, c=<span class="string">'#E65235'</span>, name=<span class="string">'samplefigure'</span>)</span>:</span></div><div class="line">    fig, ax = plt.subplots(figsize=(<span class="number">10</span>, <span class="number">40</span>))</div><div class="line"></div><div class="line">    l = len(CountList)</div><div class="line">    ax.barh(range(l), CountList, color=c, align=<span class="string">'center'</span>)</div><div class="line">    ax.set_title(<span class="string">'Top'</span> + str(l) + <span class="string">'Cited Paper'</span>, fontsize=<span class="number">14</span>)</div><div class="line">    ax.set_yticks(range(l))</div><div class="line">    ax.set_ylim([<span class="number">0</span>, l])</div><div class="line">    ax.set_yticklabels(TitleList, fontsize=<span class="number">14</span>)</div><div class="line">    ax.set_xlabel(<span class="string">'Citation Count'</span>)</div><div class="line"></div><div class="line"><span class="comment">#     plt.show()</span></div><div class="line"></div><div class="line">    fig.savefig(<span class="string">'img/'</span>+name, bbox_inches=<span class="string">'tight'</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 正式开始画</span></div><div class="line">drawMostCitedPaper(ml_top.CitedCount.tolist()[:<span class="number">-100</span>:<span class="number">-1</span>], ml_top100.Title.tolist()[::<span class="number">-1</span>], name=<span class="string">'ml'</span>)</div><div class="line">drawMostCitedPaper(dl_top.CitedCount.tolist()[:<span class="number">-100</span>:<span class="number">-1</span>], dl_top100.Title.tolist()[::<span class="number">-1</span>], c=<span class="string">'#00D680'</span>, name=<span class="string">'dl'</span>)</div><div class="line">drawMostCitedPaper(dm_top.CitedCount.tolist()[:<span class="number">-100</span>:<span class="number">-1</span>], dm_top100.Title.tolist()[::<span class="number">-1</span>], c=<span class="string">'#D866D6'</span>, name=<span class="string">'dm'</span>)</div><div class="line">drawMostCitedPaper(nn_top.CitedCount.tolist()[:<span class="number">-100</span>:<span class="number">-1</span>], nn_top100.Title.tolist()[::<span class="number">-1</span>], c=<span class="string">'#EBC12B'</span>, name=<span class="string">'nn'</span>)</div></pre></td></tr></table></figure>
<p>图：</p>
<p><img src="https://www.dropbox.com/s/kwi0f5a896k85f3/ml.png?raw=1" alt="Machine Learning Top 100 Cited Papers"><br><img src="https://www.dropbox.com/s/llhlrubjr9ze1w5/dm.png?raw=1" alt="Data Mining Top 100 Cited Papers"><br><img src="https://www.dropbox.com/s/1fs2co2ls23p5a2/dl.png?raw=1" alt="Deep Learning Top 100 Cited Papers"><br><img src="https://www.dropbox.com/s/4fcw66dpvg08kmn/nn.png?raw=1" alt="Neural Network Top 100 Cited Papers"></p>
<h4 id="画出最多引用的作者"><a href="#画出最多引用的作者" class="headerlink" title="画出最多引用的作者"></a>画出最多引用的作者</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">drawTop100CitedPerson</span><span class="params">(topic=<span class="string">'ml'</span>, c=<span class="string">'#E65235'</span>, name=<span class="string">'samplefigure'</span>)</span>:</span></div><div class="line"></div><div class="line">    df = dfs[topic]</div><div class="line"></div><div class="line">    keywords = &#123;</div><div class="line">        <span class="string">'ml'</span> : <span class="string">'Machine Learning'</span>,</div><div class="line">        <span class="string">'nn'</span> : <span class="string">'Neural Network'</span>,</div><div class="line">        <span class="string">'dm'</span> : <span class="string">'Data Mining'</span>,</div><div class="line">        <span class="string">'dl'</span> : <span class="string">'Deep Learning'</span>,</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    authordf = df.groupby([<span class="string">'Author'</span>]).CitedCount.sum()   <span class="comment"># 重要：对group之后的DataFrame执行求和操作</span></div><div class="line">    authordf = authordf.sort_values(ascending=<span class="string">'True'</span>)</div><div class="line"></div><div class="line">    CountList = list(reversed(authordf[:<span class="number">-101</span>:<span class="number">-1</span>].tolist()))</div><div class="line">    NameList = list(reversed(authordf.index.values.tolist()[:<span class="number">-101</span>:<span class="number">-1</span>]))</div><div class="line"></div><div class="line">    Namelist = [unicode(x, errors=<span class="string">'replace'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> Namelist]</div><div class="line"></div><div class="line">    fig, ax = plt.subplots(figsize=(<span class="number">10</span>, <span class="number">40</span>))</div><div class="line"></div><div class="line">    l = len(CountList)</div><div class="line">    ax.barh(range(l), CountList, color=c, align=<span class="string">'center'</span>)</div><div class="line">    ax.set_title(keywords[topic] +<span class="string">' Top '</span> + str(l) + <span class="string">' Cited Person'</span>, fontsize=<span class="number">14</span>)</div><div class="line">    ax.set_yticks(range(l))</div><div class="line">    ax.set_ylim([<span class="number">0</span>, l])</div><div class="line">    ax.set_yticklabels(NameList, fontsize=<span class="number">14</span>)</div><div class="line">    ax.set_xlabel(<span class="string">'Citation Count'</span>)</div><div class="line"></div><div class="line">    plt.show()</div><div class="line"></div><div class="line">    fig.savefig(<span class="string">'img/'</span>+name, bbox_inches=<span class="string">'tight'</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 正式开始作图</span></div><div class="line">drawTop100CitedPerson(topic=<span class="string">'ml'</span>, c=<span class="string">'#49C12B'</span>, name=<span class="string">'ml_person'</span>)</div><div class="line">drawTop100CitedPerson(topic=<span class="string">'nn'</span>, c=<span class="string">'#EBC12B'</span>, name=<span class="string">'nn_person'</span>)</div><div class="line">drawTop100CitedPerson(topic=<span class="string">'dm'</span>, c=<span class="string">'#D866D6'</span>, name=<span class="string">'dm_person'</span>)</div><div class="line">drawTop100CitedPerson(topic=<span class="string">'dl'</span>, c=<span class="string">'#00D680'</span>, name=<span class="string">'dl_person'</span>)</div></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="https://www.dropbox.com/s/azt7mt00ybg0zr4/ml_person.png?raw=1" alt="Machine Learning Top 100 Cited Author"><br><img src="https://www.dropbox.com/s/80jn4nqgx6x4r2t/dm_person.png?raw=1" alt="Data Mining Top 100 Cited Author"><br><img src="https://www.dropbox.com/s/m295o3ajxt2kada/dl_person.png?raw=1" alt="Deep Learning Top 100 Cited Author"><br><img src="https://www.dropbox.com/s/522ubgw63n3axql/nn_person.png?raw=1" alt="Neural Network Top 100 Cited Author"></p>
<h4 id="列出写最多产的作者"><a href="#列出写最多产的作者" class="headerlink" title="列出写最多产的作者"></a>列出写最多产的作者</h4><p>因为代码和最多引用作者大同小异，所以在下面就不列出来了，有兴趣的朋友可以在文末找到链接。</p>
<p>结果是这样：</p>
<p><img src="https://www.dropbox.com/s/nb0q5ofzd262tq1/ml_prolific_author.png?raw=1" alt="Machine Learning Top 100 Prolific Author"><br><img src="https://www.dropbox.com/s/pp25ql47iytq3en/dm_prolific_author.png?raw=1" alt="Data Mining Top 100 Prolific Author"><br><img src="https://www.dropbox.com/s/agj3ursey1yapxu/dl_prolific_author.png?raw=1" alt="Deep Learning Top 100 Prolific Author"><br><img src="https://www.dropbox.com/s/tb7yueo0e8544if/nn_prolific_author.png?raw=1" alt="Neural Network Top 100 Prolific Author"></p>
<p>有意思的一点是，在Machine Learning领域，前100多产的作者里面，<strong>有71位都是华人</strong>，但是在前100被引用次数最多的作者里面，<strong>只有27位是华人</strong>。</p>
<h4 id="列出被引用最多的出版商"><a href="#列出被引用最多的出版商" class="headerlink" title="列出被引用最多的出版商"></a>列出被引用最多的出版商</h4><p><img src="https://www.dropbox.com/s/v2lr60d4z39bosn/ml_journal.png?raw=1" alt="Machine Learning Top 100 Prolific Author"><br><img src="https://www.dropbox.com/s/u3vuo0mjy0pp8c9/dm_journal.png?raw=1" alt="Data Mining Top 100 Prolific Author"><br><img src="https://www.dropbox.com/s/p0hmft1strhctwi/dl_journal.png?raw=1" alt="Deep Learning Top 100 Prolific Author"><br><img src="https://www.dropbox.com/s/pfhycj6ptr5macq/nn_journal.png?raw=1" alt="Neural Network Top 100 Prolific Author"></p>
<h4 id="标题里的词频分析"><a href="#标题里的词频分析" class="headerlink" title="标题里的词频分析"></a>标题里的词频分析</h4><p>我想知道在这二十几万篇论文标题里面出现次数最多的词语是什么。词频很好计算，用我们刚刚的DataFrame，改一下过滤条件，再排个序，就可以做到。<br>为了把频率以直观的方式表现出来，我选择用 <em>词云</em> 的形式。</p>
<p>Python里有一个专门做词云的包<a href="https://github.com/amueller/word_cloud" target="_blank" rel="external">WordCloud</a>， 简单易用。想要知道更多关于使用WordCloud的信息，可以参见我<a href="https://zhuanlan.zhihu.com/p/21807428" target="_blank" rel="external">上一篇博客</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 分词并获得词根，以此来累计频率</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getWordCount</span><span class="params">(df)</span>:</span></div><div class="line">    dflist = df.Abstract.tolist()</div><div class="line"></div><div class="line"><span class="comment">#     tagged_corpus = [pos_tag(word_tokenize(document)) for document in dflist]</span></div><div class="line">    tagged_corpus = []</div><div class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dflist:</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            tmp = word_tokenize(document)</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        tagged_corpus.append(pos_tag(tmp))</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lemmatize</span><span class="params">(token, tag)</span>:</span></div><div class="line">        <span class="keyword">if</span> tag.lower() <span class="keyword">in</span> [<span class="string">'n'</span>, <span class="string">'v'</span>]:</div><div class="line">            <span class="keyword">return</span> lemmatizer.lemmatize(token, tag)</div><div class="line">        <span class="keyword">return</span> token</div><div class="line">    lemmatizer = WordNetLemmatizer()</div><div class="line"></div><div class="line">    lemmatized_corpus = [[lemmatize(token, tag) <span class="keyword">for</span> token, tag <span class="keyword">in</span> document] <span class="keyword">for</span> document <span class="keyword">in</span> tagged_corpus]</div><div class="line"></div><div class="line">    tmp = []</div><div class="line">    <span class="keyword">for</span> title <span class="keyword">in</span> lemmatized_corpus:</div><div class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> title:</div><div class="line">            tmp.append(word.lower())</div><div class="line"></div><div class="line">    lemmatized_corpus = tmp</div><div class="line"></div><div class="line">    mystopwords = [</div><div class="line">        <span class="string">','</span>,</div><div class="line">        <span class="string">':'</span>,</div><div class="line">        <span class="string">';'</span>,</div><div class="line">        <span class="string">'.'</span>,</div><div class="line">        <span class="string">'('</span>,</div><div class="line">        <span class="string">')'</span>,</div><div class="line">        <span class="string">'['</span>,</div><div class="line">        <span class="string">']'</span>,</div><div class="line">        <span class="string">"'s"</span>,</div><div class="line">        <span class="string">'-'</span>,</div><div class="line">        <span class="string">'?'</span>,</div><div class="line">        <span class="string">"'"</span>,</div><div class="line">        <span class="string">'%'</span>,</div><div class="line">        <span class="string">'&amp;'</span>,</div><div class="line">        <span class="string">'...'</span>,</div><div class="line">        <span class="string">'…'</span>,</div><div class="line">        <span class="string">'.'</span></div><div class="line"></div><div class="line">    ]</div><div class="line"></div><div class="line"></div><div class="line">    count = Counter(tmp).items()</div><div class="line">    count = sorted(count, key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse=<span class="keyword">True</span>)</div><div class="line">    count = [(w, c) <span class="keyword">for</span> w, c <span class="keyword">in</span> count <span class="keyword">if</span> w.lower() <span class="keyword">not</span> <span class="keyword">in</span> stopwords.words(<span class="string">'english'</span>) <span class="keyword">and</span> w <span class="keyword">not</span> <span class="keyword">in</span> mystopwords]</div><div class="line">    <span class="keyword">return</span> count</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义词云函数</span></div><div class="line"><span class="comment">#-*- coding: utf-8 -*-</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">drawWordCloud</span><span class="params">(frequency, name=<span class="string">'samplecloud'</span>)</span> :</span></div><div class="line">    <span class="keyword">from</span> scipy.misc <span class="keyword">import</span> imread</div><div class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">    <span class="keyword">from</span> progressbar <span class="keyword">import</span> ProgressBar</div><div class="line">    <span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud, ImageColorGenerator</div><div class="line">    %matplotlib notebook</div><div class="line"></div><div class="line"></div><div class="line">    fig, ax = plt.subplots(figsize=(<span class="number">40</span>, <span class="number">40</span>), dpi=<span class="number">60</span>)</div><div class="line">    img_colors = ImageColorGenerator(img_mask)</div><div class="line"></div><div class="line">    wc = WordCloud(background_color=<span class="string">"white"</span>,</div><div class="line">    max_font_size=<span class="number">200</span>, <span class="comment">#字体最大值</span></div><div class="line">    random_state=<span class="number">2</span>,</div><div class="line">    relative_scaling=<span class="number">1</span>,)</div><div class="line"><span class="comment">#     prefer_horizontal=True )</span></div><div class="line"><span class="comment">#     ranks_only=True)</span></div><div class="line"></div><div class="line">    wc.fit_words(frequency)</div><div class="line"></div><div class="line">    plt.imshow(wc)</div><div class="line">    plt.axis(<span class="string">'off'</span>)</div><div class="line"></div><div class="line">    plt.show()</div><div class="line">    plt.savefig(<span class="string">'img/'</span>+name)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 正式画图</span></div><div class="line"><span class="keyword">for</span> topic <span class="keyword">in</span> dfs:</div><div class="line">    df = dfs[topic]</div><div class="line">    count = getWordCount(df)</div><div class="line">    drawWordCloud(count, name=topic + <span class="string">'_wordcloud'</span>)</div></pre></td></tr></table></figure>
<p>下面是结果：</p>
<p>Machine Learning:</p>
<p><img src="https://www.dropbox.com/s/mifsvubh7ceohn6/ml_wordcloud06.png?raw=1" alt=""></p>
<p>Data Mining:</p>
<p><img src="https://www.dropbox.com/s/7df9hwe7vpqii1z/dm_wordcloud05.png?raw=1" alt=""></p>
<p>Deep Learning:</p>
<p><img src="https://www.dropbox.com/s/dj27rcxptfp11gv/dl_wordcloud05.png?raw=1" alt=""></p>
<p>Neural Networks:</p>
<p><img src="https://www.dropbox.com/s/5tq1rpdp7slo82v/nn_wordcloud06.png?raw=1" alt=""></p>
<p>源代码：<a href="https://www.dropbox.com/s/j0zh9v5u5zpz1kf/main.ipynb" target="_blank" rel="external">爬虫</a> 和 <a href="https://www.dropbox.com/s/fdm7ull5g6v5z7i/%E5%A4%84%E7%90%86.ipynb" target="_blank" rel="external">处理</a></p>
<p>谢绝转载</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/08/15/Machine-Learning-Scholar-Rank/" data-id="ciuy2tjjt0000ysayhbe9ixr5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jupyter-Notebook-Environment-Anaconda/">Jupyter Notebook Environment Anaconda</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Jupyter-Notebook-Environment-Anaconda/" style="font-size: 10px;">Jupyter Notebook Environment Anaconda</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/10/31/question-diff-privacy/">question_diff_privacy</a>
          </li>
        
          <li>
            <a href="/2016/09/26/data-sceince-portfolio-machine-learning-project/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/09/21/data-sceince-portfolio-machine-learning-project/">构建数据科学的作品集：从机器学习项目开始</a>
          </li>
        
          <li>
            <a href="/2016/09/18/the_annoying_jupyter_notebook/">jupyter notebook调用不同environment解决方案</a>
          </li>
        
          <li>
            <a href="/2016/09/18/SAME_is_not_same_in_tf/">tf.nn.conv2d里的padding解析</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Tang Xiaoting<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>